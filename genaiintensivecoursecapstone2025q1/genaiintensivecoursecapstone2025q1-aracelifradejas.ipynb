{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae62c544",
   "metadata": {
    "papermill": {
     "duration": 0.009211,
     "end_time": "2025-04-12T13:45:03.581878",
     "exception": false,
     "start_time": "2025-04-12T13:45:03.572667",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 👟 GenAI Customer Feedback Assistant for KelceTS S.L. -  GenAI Intensive Course Capstone Project 2025Q1-Araceli Fradejas Muñoz\n",
    "This project presents a complete Generative AI pipeline to automate customer service analysis for **KelceTS S.L.**, a fictional European sneaker e-commerce brand. It was created as part of the **Kaggle + Google GenAI Intensive Capstone 2025Q1**.\n",
    "\n",
    "## 🔍 Objective\n",
    "Customers often leave reviews that the quality team must manually evaluate. These reviews must be analyzed to:\n",
    "- Detect product defects or expectations not met\n",
    "- Respond empathetically and brand-aligned in the customer's language\n",
    "- Notify internal teams (logistics, quality) or suppliers with proper actions\n",
    "\n",
    "The goal was to **build a modular and intelligent assistant** capable of:\n",
    "- Interpreting customer comments (multilingual)\n",
    "- Responding with empathy and brand tone\n",
    "- Triggering internal or external communications based on internal rules\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Capabilities Used\n",
    "This notebook demonstrates **7 GenAI capabilities**:\n",
    "- ✅ Structured Output in JSON\n",
    "- ✅ Retrieval Augmented Generation (RAG)\n",
    "- ✅ Embeddings\n",
    "- ✅ Agents (LangGraph modular workflow)\n",
    "- ✅ Function Calling\n",
    "- ✅ GenAI Evaluation (auto-score vs rubric)\n",
    "- ✅ Document Understanding (via rules XLS)\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 Approach Overview\n",
    "1. **Load internal rules** from Excel files as context\n",
    "2. **Create vector store** for retrieval using Gemini Embeddings + FAISS\n",
    "3. **User prompt** → comment + context → Gemini model\n",
    "4. **Generate structured JSON** with:\n",
    "   - Analysis summary\n",
    "   - Customer response\n",
    "   - Internal and provider emails\n",
    "5. **Evaluate response** on:\n",
    "   - Groundedness\n",
    "   - Relevance\n",
    "   - Empathy/tone\n",
    "\n",
    "6. **Compare Standard vs RAG-enhanced output**\n",
    "7. **Show modular LangGraph workflow**\n",
    "\n",
    "---\n",
    "\n",
    "## 💬 Example Output\n",
    "A German customer complains about quality after 2.5 months.\n",
    "\n",
    "The assistant:\n",
    "- Detects the issue\n",
    "- Responds empathetically in German\n",
    "- Prepares emails for internal quality control and the external provider\n",
    "\n",
    "---\n",
    "\n",
    "## 📈 Results & Learnings\n",
    "This project shows how **Generative AI + internal rules + modular orchestration** can improve customer support at scale.\n",
    "\n",
    "It also shows how LangGraph allows agents to be extended with:\n",
    "- Quality factors\n",
    "- Evaluation\n",
    "- Export\n",
    "\n",
    "> This solution is scalable, multilingual, modular, and aligned with enterprise communication standards.\n",
    "\n",
    "---\n",
    "\n",
    "Created by **Araceli Fradejas**  \n",
    "April 12 2025  \n",
    "GenAI Intensive Course Capstone – Kaggle + Google\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cf6da4",
   "metadata": {
    "papermill": {
     "duration": 0.007901,
     "end_time": "2025-04-12T13:45:03.598305",
     "exception": false,
     "start_time": "2025-04-12T13:45:03.590404",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 📑 Notebook Index\n",
    "\n",
    "1. Install & Setup\n",
    "2. Load Data & Rules\n",
    "3. Process a Customer Comment with Gemini .\n",
    "   3.1 Internal Policy Consultation via Retrieval\n",
    "   3.2 Modular Response Flow with LangGraph\n",
    "   3.3 Visual LangGraph Diagram with Quality Nodes\n",
    "4. Create a Test Message and Interpret Output\n",
    "5. Export Output to Table or Excel\n",
    "6. Automatic Evaluation of Agent Responses\n",
    "7. Conclusions & Future Work\n",
    "8. Submission Summary & Acknowledgements\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddb7bdc",
   "metadata": {
    "papermill": {
     "duration": 0.00909,
     "end_time": "2025-04-12T13:45:03.615656",
     "exception": false,
     "start_time": "2025-04-12T13:45:03.606566",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 📘 Section 1: Setup\n",
    "Install the needed libraries and prepare the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1181a71b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T13:45:03.633005Z",
     "iopub.status.busy": "2025-04-12T13:45:03.632661Z",
     "iopub.status.idle": "2025-04-12T13:45:39.880764Z",
     "shell.execute_reply": "2025-04-12T13:45:39.879290Z"
    },
    "papermill": {
     "duration": 36.259397,
     "end_time": "2025-04-12T13:45:39.883080",
     "exception": false,
     "start_time": "2025-04-12T13:45:03.623683",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.6/179.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.4/175.4 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m423.3/423.3 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# Install all required packages with versions compatible in Kaggle\n",
    "!pip install -Uq \\\n",
    "  google-generativeai \\\n",
    "  langchain \\\n",
    "  langchain-community \\\n",
    "  langchain-google-genai \\\n",
    "  fsspec==2024.10.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d338f6c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T13:45:39.902543Z",
     "iopub.status.busy": "2025-04-12T13:45:39.902183Z",
     "iopub.status.idle": "2025-04-12T13:45:46.490566Z",
     "shell.execute_reply": "2025-04-12T13:45:46.489100Z"
    },
    "papermill": {
     "duration": 6.600278,
     "end_time": "2025-04-12T13:45:46.492924",
     "exception": false,
     "start_time": "2025-04-12T13:45:39.892646",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\r\n",
      "  Downloading faiss_cpu-1.10.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\r\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\r\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.2)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2025.0.1)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2022.0.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2022.0.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\r\n",
      "Downloading faiss_cpu-1.10.0-cp310-cp310-manylinux_2_28_x86_64.whl (30.7 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: faiss-cpu\r\n",
      "Successfully installed faiss-cpu-1.10.0\r\n"
     ]
    }
   ],
   "source": [
    "# Install FAISS separately (needed for vector store)\n",
    "!pip install faiss-cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "808120da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T13:45:46.513932Z",
     "iopub.status.busy": "2025-04-12T13:45:46.513556Z",
     "iopub.status.idle": "2025-04-12T13:45:50.860887Z",
     "shell.execute_reply": "2025-04-12T13:45:50.859150Z"
    },
    "papermill": {
     "duration": 4.360459,
     "end_time": "2025-04-12T13:45:50.863302",
     "exception": false,
     "start_time": "2025-04-12T13:45:46.502843",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (0.20.3)\r\n"
     ]
    }
   ],
   "source": [
    " #Install Graphviz (needed to render visual LangGraph diagrams)\n",
    "!pip install graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b675cce9",
   "metadata": {
    "papermill": {
     "duration": 0.01144,
     "end_time": "2025-04-12T13:45:50.887892",
     "exception": false,
     "start_time": "2025-04-12T13:45:50.876452",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 📘 Section 2: Load Data & Rules\n",
    "In this section, we will configure the Google Generative AI API by setting up the API key and verifying the available models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3240cfbd",
   "metadata": {
    "papermill": {
     "duration": 0.013122,
     "end_time": "2025-04-12T13:45:50.912753",
     "exception": false,
     "start_time": "2025-04-12T13:45:50.899631",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.1 Import Libraries and Configure API Key\n",
    "The first step is to import the necessary libraries and configure the API key. We retrieve the API key from the environment variable (`GOOGLE_API_KEY`) which is securely stored in the Kaggle Secrets. This ensures that our API key is available for authentication when making API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dc14525",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T13:45:50.934211Z",
     "iopub.status.busy": "2025-04-12T13:45:50.933830Z",
     "iopub.status.idle": "2025-04-12T13:45:53.950025Z",
     "shell.execute_reply": "2025-04-12T13:45:53.948904Z"
    },
    "papermill": {
     "duration": 3.029223,
     "end_time": "2025-04-12T13:45:53.952134",
     "exception": false,
     "start_time": "2025-04-12T13:45:50.922911",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003ac003",
   "metadata": {
    "papermill": {
     "duration": 0.009462,
     "end_time": "2025-04-12T13:45:53.971898",
     "exception": false,
     "start_time": "2025-04-12T13:45:53.962436",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.2 Import Libraries and Configure API Key\n",
    "Before proceeding with generating any content, it is important to list the available models. This helps to confirm whether the desired model (such as \"models/gemini-1.0-pro\") is available for use. If the model is not available, you may need to select one of the models that are returned from this call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4bc719b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T13:45:53.992069Z",
     "iopub.status.busy": "2025-04-12T13:45:53.991708Z",
     "iopub.status.idle": "2025-04-12T13:45:54.100903Z",
     "shell.execute_reply": "2025-04-12T13:45:54.099435Z"
    },
    "papermill": {
     "duration": 0.121546,
     "end_time": "2025-04-12T13:45:54.102740",
     "exception": false,
     "start_time": "2025-04-12T13:45:53.981194",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Gemini API configured successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load the Gemini API Key securely using Kaggle Secrets\n",
    "GOOGLE_API_KEY = os.environ.get(\"GOOGLE_API_KEY\", None)\n",
    "\n",
    "if GOOGLE_API_KEY is None:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# Configure Gemini\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "print(\"✅ Gemini API configured successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e114399",
   "metadata": {
    "papermill": {
     "duration": 0.009309,
     "end_time": "2025-04-12T13:45:54.121719",
     "exception": false,
     "start_time": "2025-04-12T13:45:54.112410",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.3 Select and Use a Compatible Model\n",
    "After verifying the available models, choose a model that is available. For example, if \"models/gemini-1.0-pro\" is not in the list, you may select \"models/gemini-1.0\" if it appears. You can then generate content using the selected model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13e07c30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T13:45:54.141970Z",
     "iopub.status.busy": "2025-04-12T13:45:54.141531Z",
     "iopub.status.idle": "2025-04-12T13:45:57.905604Z",
     "shell.execute_reply": "2025-04-12T13:45:57.904207Z"
    },
    "papermill": {
     "duration": 3.775987,
     "end_time": "2025-04-12T13:45:57.907279",
     "exception": false,
     "start_time": "2025-04-12T13:45:54.131292",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models:\n",
      "- models/chat-bison-001\n",
      "- models/text-bison-001\n",
      "- models/embedding-gecko-001\n",
      "- models/gemini-1.0-pro-vision-latest\n",
      "- models/gemini-pro-vision\n",
      "- models/gemini-1.5-pro-latest\n",
      "- models/gemini-1.5-pro-001\n",
      "- models/gemini-1.5-pro-002\n",
      "- models/gemini-1.5-pro\n",
      "- models/gemini-1.5-flash-latest\n",
      "- models/gemini-1.5-flash-001\n",
      "- models/gemini-1.5-flash-001-tuning\n",
      "- models/gemini-1.5-flash\n",
      "- models/gemini-1.5-flash-002\n",
      "- models/gemini-1.5-flash-8b\n",
      "- models/gemini-1.5-flash-8b-001\n",
      "- models/gemini-1.5-flash-8b-latest\n",
      "- models/gemini-1.5-flash-8b-exp-0827\n",
      "- models/gemini-1.5-flash-8b-exp-0924\n",
      "- models/gemini-2.5-pro-exp-03-25\n",
      "- models/gemini-2.5-pro-preview-03-25\n",
      "- models/gemini-2.0-flash-exp\n",
      "- models/gemini-2.0-flash\n",
      "- models/gemini-2.0-flash-001\n",
      "- models/gemini-2.0-flash-exp-image-generation\n",
      "- models/gemini-2.0-flash-lite-001\n",
      "- models/gemini-2.0-flash-lite\n",
      "- models/gemini-2.0-flash-lite-preview-02-05\n",
      "- models/gemini-2.0-flash-lite-preview\n",
      "- models/gemini-2.0-pro-exp\n",
      "- models/gemini-2.0-pro-exp-02-05\n",
      "- models/gemini-exp-1206\n",
      "- models/gemini-2.0-flash-thinking-exp-01-21\n",
      "- models/gemini-2.0-flash-thinking-exp\n",
      "- models/gemini-2.0-flash-thinking-exp-1219\n",
      "- models/learnlm-1.5-pro-experimental\n",
      "- models/gemma-3-1b-it\n",
      "- models/gemma-3-4b-it\n",
      "- models/gemma-3-12b-it\n",
      "- models/gemma-3-27b-it\n",
      "- models/embedding-001\n",
      "- models/text-embedding-004\n",
      "- models/gemini-embedding-exp-03-07\n",
      "- models/gemini-embedding-exp\n",
      "- models/aqa\n",
      "- models/imagen-3.0-generate-002\n",
      "- models/veo-2.0-generate-001\n",
      "- models/gemini-2.0-flash-live-001\n",
      "Generative AI is a type of artificial intelligence that can create new content, rather than just analyzing or classifying existing data.  Instead of simply recognizing patterns, it learns those patterns and then uses that knowledge to generate similar, but entirely new, outputs.  Think of it as moving from \"understanding\" to \"creating.\"\n",
      "\n",
      "Here's a breakdown:\n",
      "\n",
      "* **Learning from data:**  Like all AI, generative AI learns from large datasets.  This could be text, images, audio, video, or code. The crucial difference is *how* it learns.  It learns the underlying structure and statistical relationships within the data, identifying patterns and probabilities.\n",
      "\n",
      "* **Generating new data:**  Once trained, the model can generate new content that resembles the training data but isn't a direct copy.  This is done through various techniques, often involving probability distributions and complex mathematical models.\n",
      "\n",
      "* **Examples of outputs:**  The type of output depends on the training data and the specific model.  This could include:\n",
      "    * **Text:**  Writing stories, articles, poems, code, scripts, etc. (e.g., ChatGPT, Bard)\n",
      "    * **Images:**  Creating realistic or stylized images, manipulating existing images (e.g., DALL-E 2, Midjourney, Stable Diffusion)\n",
      "    * **Audio:**  Composing music, generating sound effects, creating voiceovers (e.g., Jukebox)\n",
      "    * **Video:**  Generating short video clips (still early stages of development)\n",
      "    * **3D models:**  Creating three-dimensional objects\n",
      "\n",
      "* **Underlying Techniques:**  Various techniques power generative AI, including:\n",
      "    * **Generative Adversarial Networks (GANs):**  Two neural networks compete against each other – one generates content, and the other tries to distinguish it from real data.\n",
      "    * **Variational Autoencoders (VAEs):**  Learn a compressed representation of the data and then use it to generate new instances.\n",
      "    * **Transformers:**  Powerful neural network architectures particularly effective at processing sequential data like text and code, widely used in large language models.\n",
      "    * **Diffusion models:**  Gradually add noise to data and then learn to reverse the process, generating new data from noise.\n",
      "\n",
      "\n",
      "In essence, generative AI is a powerful tool that can automate creative tasks, accelerate content creation, and even generate novel ideas.  However, it's crucial to be aware of its limitations, including potential biases in the training data and the ethical implications of generating realistic but potentially misleading content.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View available models\n",
    "models = genai.list_models()\n",
    "print(\"Available models:\")\n",
    "for m in models:\n",
    "    print(f\"- {m.name}\")\n",
    "\n",
    "# Try with a model from the list\n",
    "try:\n",
    "    model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\")\n",
    "    \n",
    "    response = model.generate_content(\n",
    "        contents=[\n",
    "            {\"role\": \"user\", \"parts\": [\"Explain what generative artificial intelligence is\"]}\n",
    "        ]\n",
    "    )\n",
    "    print(response.text)\n",
    "except Exception as e:\n",
    "    print(f\"Error using the model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74279cc",
   "metadata": {
    "papermill": {
     "duration": 0.009199,
     "end_time": "2025-04-12T13:45:57.926550",
     "exception": false,
     "start_time": "2025-04-12T13:45:57.917351",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 📘 Section 3: Process a Customer Comment with Gemini\n",
    "In this section, we define the prompt that simulates an expert assistant for KelceTS' quality team. We pass a real customer comment as input and get:\n",
    "\n",
    "A JSON object containing:\n",
    "\n",
    "Extracted insights\n",
    "\n",
    "Response for the customer\n",
    "\n",
    "Internal email\n",
    "\n",
    "Provider escalation (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a6ec1ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T13:45:57.948273Z",
     "iopub.status.busy": "2025-04-12T13:45:57.947875Z",
     "iopub.status.idle": "2025-04-12T13:46:06.772173Z",
     "shell.execute_reply": "2025-04-12T13:46:06.770707Z"
    },
    "papermill": {
     "duration": 8.837937,
     "end_time": "2025-04-12T13:46:06.774125",
     "exception": false,
     "start_time": "2025-04-12T13:45:57.936188",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"customer_analysis\": {\n",
      "    \"language\": \"German\",\n",
      "    \"delivery_within_96h\": \"Not specified\",\n",
      "    \"packaging_condition\": \"Not specified\",\n",
      "    \"correct_shoe_size\": \"Yes\",\n",
      "    \"material_quality\": \"Poor\",\n",
      "    \"usage_type\": \"Daily\",\n",
      "    \"expectations_met\": \"No\"\n",
      "  },\n",
      "  \"customer_email\": \"Hallo! Vielen Dank für dein Feedback. Es tut uns sehr leid zu hören, dass deine Schuhe sich so schnell abgenutzt haben. Wir legen großen Wert auf die Qualität unserer Produkte und bedauern, dass deine Erwartungen nicht erfüllt wurden.  Wir möchten dies gerne wieder gutmachen. Bitte sende uns Fotos der beschädigten Stellen an info@kelcets.com. Wir werden dir dann umgehend ein neues Paar Schuhe zusenden oder dir den Kaufpreis erstatten.  Vielen Dank für dein Verständnis.\",\n",
      "  \"internal_email\": \"Betreff: Kundenbeschwerde - Schnelle Abnutzung der Schuhe\\n\\nKundennummer: [Kundennummer einfügen]\\n\\nEin Kunde hat sich über die schnelle Abnutzung seiner Schuhe beschwert. Er gab an, die Schuhe täglich getragen zu haben, und nach nur 2,5 Monaten seien Löcher in den Sohlen und abgenutzte Stellen im Inneren sichtbar.  Der Kunde erwartet eine Ersatzlieferung oder eine Rückerstattung. Bitte prüfen Sie den Fall und veranlassen Sie die entsprechende Aktion. Fotos der Schuhe wurden vom Kunden angefordert und werden nach Erhalt an Sie weitergeleitet.\",\n",
      "  \"provider_email\": \"Sehr geehrte Damen und Herren,\\n\\nwir haben eine Kundenbeschwerde bezüglich der Haltbarkeit eines Ihrer Produkte erhalten. Der Kunde berichtet von übermäßigem Verschleiß und Materialermüdung nach nur 2,5 Monaten täglicher Nutzung.  Anbei finden Sie Fotos der beschädigten Schuhe [Fotos anhängen].  Wir bitten Sie, diesen Fall zu untersuchen und uns Ihre Einschätzung zur Ursache des Problems mitzuteilen.  Wir erwarten eine Stellungnahme Ihrerseits, wie wir in Zukunft ähnliche Qualitätsprobleme vermeiden können.  Mit freundlichen Grüßen,\\n\\nKelceTS S.L. Qualitätsmanagement\"\n",
      "}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the system instruction \n",
    "system_instruction = \"\"\"\n",
    "You are an AI assistant for the quality team of the European sneaker e-commerce startup \"KelceTS S.L.\"\n",
    "You have over 10 years of experience managing customer satisfaction and quality in top sportswear companies.\n",
    "\n",
    "Your job is to analyze customer feedback and generate:\n",
    "1. A response email for the customer, in their original language.\n",
    "2. A notification email for KelceTS' logistics and/or quality team (if negative feedback).\n",
    "3. An email for external providers (if needed), in formal tone.\n",
    "\n",
    "You must extract the following values from the feedback:\n",
    "- Language used\n",
    "- Delivery within 96h?\n",
    "- Packaging condition\n",
    "- Correct shoe size?\n",
    "- Material quality?\n",
    "- Usage type?\n",
    "- Expectations met?\n",
    "\n",
    "You must apply evaluation rules (previously loaded) and include:\n",
    "- Compensation actions (if negative)\n",
    "- Always use friendly tone (tú), except for providers\n",
    "- Always output JSON for structured export to Excel\n",
    "\n",
    "You must generate a structured dictionary like:\n",
    "\n",
    "{\n",
    " \"customer_analysis\": {...},\n",
    " \"customer_email\": \"...\",\n",
    " \"internal_email\": \"...\",\n",
    " \"provider_email\": \"...\"\n",
    "}\n",
    "\n",
    "Your temperature setting is 0.0. You must follow these instructions step by step.\n",
    "\"\"\"\n",
    "\n",
    "# Define the user input (customer feedback)\n",
    "comment_1 = \"\"\"\n",
    "Als die Schuhe angekommen sind habe ich mich erstmal sehr gefreut und hätte wahrscheinlich 5 Sterne vergeben! Sie haben gut gepasst, wirkten auf den ersten Blick relativ gut verarbeitet und sehr bequem. Leider musste ich fast sofort feststellen dass die Schuhe sich unnormal schnell abnutzen. Bereits nach 2.5 Monaten (von - zugegeben - fast täglicher Benutzung) sind die Schuhe nicht mehr zu gebrauchen. Sie haben innen abgenutzte Stellen wo rohes Plastik durch guckt, beide haben jeweils ein Loch in der Sohle.\n",
    "\"\"\"\n",
    "\n",
    "# Define generation settings\n",
    "generation_config = genai.types.GenerationConfig(\n",
    "    temperature=0.0,\n",
    "    top_p=1,\n",
    "    top_k=1,\n",
    "    max_output_tokens=2048,\n",
    ")\n",
    "\n",
    "# Load model and generate content\n",
    "model = genai.GenerativeModel(model_name=\"models/gemini-1.5-pro\")  # Cambiado a gemini-1.5-pro\n",
    "\n",
    "try:\n",
    "    response = model.generate_content(\n",
    "        contents=[\n",
    "            {\"role\": \"user\", \"parts\": [system_instruction]},\n",
    "            {\"role\": \"user\", \"parts\": [comment_1]}\n",
    "        ],\n",
    "        generation_config=generation_config\n",
    "    )\n",
    "# Show output\n",
    "    print(response.text)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1e171e",
   "metadata": {
    "papermill": {
     "duration": 0.009307,
     "end_time": "2025-04-12T13:46:06.793820",
     "exception": false,
     "start_time": "2025-04-12T13:46:06.784513",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 📘 Section 3.1: Internal Policy Consultation via Retrieval\n",
    "To ensure that responses generated by the assistant are aligned with internal policies, this section incorporates a lightweight retrieval step.\n",
    "\n",
    "Before generating the final response, the assistant is given access to internal documents—such as customer service protocols or quality rules—and retrieves relevant entries based on the customer's comment.\n",
    "\n",
    "This enables the assistant to base its answers on consistent internal criteria, ensuring that communication reflects company policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22105d6f",
   "metadata": {
    "papermill": {
     "duration": 0.009288,
     "end_time": "2025-04-12T13:46:06.813068",
     "exception": false,
     "start_time": "2025-04-12T13:46:06.803780",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### ✅ Step 1: Load all policy spreadsheets and convert to plain text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35374072",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T13:46:06.835099Z",
     "iopub.status.busy": "2025-04-12T13:46:06.834729Z",
     "iopub.status.idle": "2025-04-12T13:46:06.848296Z",
     "shell.execute_reply": "2025-04-12T13:46:06.846725Z"
    },
    "papermill": {
     "duration": 0.027324,
     "end_time": "2025-04-12T13:46:06.850214",
     "exception": false,
     "start_time": "2025-04-12T13:46:06.822890",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Kelse TS Data Kaggle.txt', 'Reglas de calidad clientes KelceTS SL.xlsx', 'Reglas de medidas de calidad KelceTS SL.xlsx', 'Reglas de como valorar un comentario KelceTS SL.xlsx', 'Reglas de comunicaciones equipos calidad y logstica KelceTS SL.xlsx']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Show exact file names available in the dataset\n",
    "print(os.listdir(\"/kaggle/input/kelse-ts-data\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2487702e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T13:46:06.871456Z",
     "iopub.status.busy": "2025-04-12T13:46:06.871083Z",
     "iopub.status.idle": "2025-04-12T13:46:08.584241Z",
     "shell.execute_reply": "2025-04-12T13:46:08.582914Z"
    },
    "papermill": {
     "duration": 1.72622,
     "end_time": "2025-04-12T13:46:08.586502",
     "exception": false,
     "start_time": "2025-04-12T13:46:06.860282",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load each spreadsheet using its exact name (including the typo \"logstica\")\n",
    "df_1 = pd.read_excel(\"/kaggle/input/kelse-ts-data/Reglas de calidad clientes KelceTS SL.xlsx\")\n",
    "df_2 = pd.read_excel(\"/kaggle/input/kelse-ts-data/Reglas de medidas de calidad KelceTS SL.xlsx\")\n",
    "df_3 = pd.read_excel(\"/kaggle/input/kelse-ts-data/Reglas de como valorar un comentario KelceTS SL.xlsx\")\n",
    "df_4 = pd.read_excel(\"/kaggle/input/kelse-ts-data/Reglas de comunicaciones equipos calidad y logstica KelceTS SL.xlsx\")\n",
    "\n",
    "# Convert each DataFrame to readable plain text (row by row)\n",
    "def df_to_text(df):\n",
    "    return \"\\n\".join([\" → \".join(map(str, row)) for row in df.values])\n",
    "\n",
    "# Combine all rules into a single text block\n",
    "rules_text = \"\\n\\n\".join([\n",
    "    df_to_text(df_1),\n",
    "    df_to_text(df_2),\n",
    "    df_to_text(df_3),\n",
    "    df_to_text(df_4)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c970e8fc",
   "metadata": {
    "papermill": {
     "duration": 0.009368,
     "end_time": "2025-04-12T13:46:08.605934",
     "exception": false,
     "start_time": "2025-04-12T13:46:08.596566",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### ✅ Step 2: Create the vector store using Gemini embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0dd7746",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T13:46:08.627288Z",
     "iopub.status.busy": "2025-04-12T13:46:08.626611Z",
     "iopub.status.idle": "2025-04-12T13:46:09.720175Z",
     "shell.execute_reply": "2025-04-12T13:46:09.719149Z"
    },
    "papermill": {
     "duration": 1.106478,
     "end_time": "2025-04-12T13:46:09.722300",
     "exception": false,
     "start_time": "2025-04-12T13:46:08.615822",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Split the rule document into smaller chunks\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "documents = splitter.create_documents([rules_text])\n",
    "\n",
    "# Create vector representations using Gemini embeddings\n",
    "embedding_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "vectorstore = FAISS.from_documents(documents, embedding_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27b890f",
   "metadata": {
    "papermill": {
     "duration": 0.010619,
     "end_time": "2025-04-12T13:46:09.743600",
     "exception": false,
     "start_time": "2025-04-12T13:46:09.732981",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### ✅ Step 3: Retrieve relevant rules and enrich the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d3b7bbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T13:46:09.764531Z",
     "iopub.status.busy": "2025-04-12T13:46:09.764096Z",
     "iopub.status.idle": "2025-04-12T13:46:13.090451Z",
     "shell.execute_reply": "2025-04-12T13:46:13.089016Z"
    },
    "papermill": {
     "duration": 3.338962,
     "end_time": "2025-04-12T13:46:13.092357",
     "exception": false,
     "start_time": "2025-04-12T13:46:09.753395",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"customer_analysis\": {\n",
      "    \"language_used\": \"German\",\n",
      "    \"delivery_within_96h\": null, \n",
      "    \"packaging_condition\": null,\n",
      "    \"correct_shoe_size\": true,\n",
      "    \"material_quality\": \"low\",\n",
      "    \"usage_type\": \"almost daily\",\n",
      "    \"expectations_met\": false\n",
      "  },\n",
      "  \"customer_email\": \"Hallo [Customer Name],\\n\\nvielen Dank für deine Nachricht und dein Feedback zu deinen KelceTS Schuhen. Es tut uns sehr leid zu hören, dass die Schuhe sich so schnell abgenutzt haben. Wir bedauern die entstandenen Unannehmlichkeiten sehr.\\n\\nWir nehmen dein Feedback sehr ernst und haben diesen Punkt als Verbesserungsmöglichkeit registriert.  Als Entschädigung für die entstandenen Unannehmlichkeiten möchten wir dir einen 25% Rabatt auf deinen nächsten Einkauf bei KelceTS anbieten, inklusive kostenloser Lieferung.\\n\\nWir werden in weniger als 72 Stunden ein Mitglied unseres Teams schicken, um die defekten Schuhe abzuholen. Bitte halte sie dafür in der Originalverpackung oder einer ähnlichen Verpackung bereit.\\n\\nWir bedanken uns für dein Verständnis und deine Geduld.\\n\\nMit freundlichen Grüßen,\\n\\nDas KelceTS Team\",\n",
      "  \"internal_email\": \"Betreff: Qualitätsproblem - Schuhe [Customer Name] - [Order Number]\\n\\nHallo Team,\\n\\nEin Kunde hat sich über die schlechte Qualität seiner Schuhe beschwert.  Die Schuhe haben sich nach nur 2,5 Monaten intensiver Nutzung abgenutzt und weisen Löcher in der Sohle und abgenutzte Stellen auf, an denen das Plastik sichtbar ist.\\n\\nBitte organisiert die Abholung der Schuhe innerhalb von 72 Stunden beim Kunden [Customer Name] unter [Customer Address].  Der Kunde hat einen 25% Rabatt auf seine nächste Bestellung erhalten und die Versandkosten wurden übernommen.\\n\\nBitte dokumentiert den Fall gründlich und analysiert die Ursache des Problems. \\n\\nVielen Dank,\\n\\nDas KelceTS Qualitätsteam\",\n",
      "  \"provider_email\": null\n",
      "}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Retrieve relevant policy context based on the customer's comment\n",
    "query = comment_1  # this is the same customer comment from earlier section\n",
    "relevant_docs = vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "# Combine retrieved context into a single string\n",
    "context = \"\\n\".join([doc.page_content for doc in relevant_docs])\n",
    "\n",
    "# Build final prompt including system instructions, customer comment, and internal context\n",
    "rag_prompt = f\"\"\"\n",
    "Use the following internal policy context to interpret the customer feedback and generate a complete response aligned with KelceTS S.L. standards.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Customer Comment:\n",
    "{comment_1}\n",
    "\n",
    "Instructions:\n",
    "{system_instruction}\n",
    "\"\"\"\n",
    "\n",
    "# Call Gemini model with the enriched prompt\n",
    "model = genai.GenerativeModel(model_name=\"models/gemini-1.5-flash\")\n",
    "response_rag = model.generate_content(rag_prompt)\n",
    "\n",
    "# Display the enriched response\n",
    "print(response_rag.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1663edb",
   "metadata": {
    "papermill": {
     "duration": 0.009913,
     "end_time": "2025-04-12T13:46:13.112887",
     "exception": false,
     "start_time": "2025-04-12T13:46:13.102974",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 📘 Section 3.2: Modular Response Flow with LangGraph\n",
    "\n",
    "To demonstrate the use of agent orchestration and flow control, we use [LangGraph](https://docs.langgraph.dev) to structure our customer feedback response pipeline into a simple graph.\n",
    "\n",
    "The graph defines two core nodes:\n",
    "1. **RetrieveContext** — Retrieves relevant rules using vector search\n",
    "2. **GenerateResponse** — Uses Gemini to generate a full response using the retrieved context\n",
    "\n",
    "This modular approach reflects how production-grade agents can be structured for robustness and traceability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c584aa66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T13:46:13.135096Z",
     "iopub.status.busy": "2025-04-12T13:46:13.134628Z",
     "iopub.status.idle": "2025-04-12T13:46:22.912243Z",
     "shell.execute_reply": "2025-04-12T13:46:22.910622Z"
    },
    "papermill": {
     "duration": 9.790908,
     "end_time": "2025-04-12T13:46:22.914199",
     "exception": false,
     "start_time": "2025-04-12T13:46:13.123291",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langgraph\r\n",
      "  Downloading langgraph-0.3.29-py3-none-any.whl.metadata (7.7 kB)\r\n",
      "Requirement already satisfied: langchain-core<0.4,>=0.1 in /usr/local/lib/python3.10/dist-packages (from langgraph) (0.3.51)\r\n",
      "Collecting langgraph-checkpoint<3.0.0,>=2.0.10 (from langgraph)\r\n",
      "  Downloading langgraph_checkpoint-2.0.24-py3-none-any.whl.metadata (4.6 kB)\r\n",
      "Collecting langgraph-prebuilt<0.2,>=0.1.1 (from langgraph)\r\n",
      "  Downloading langgraph_prebuilt-0.1.8-py3-none-any.whl.metadata (5.0 kB)\r\n",
      "Collecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph)\r\n",
      "  Downloading langgraph_sdk-0.1.61-py3-none-any.whl.metadata (1.8 kB)\r\n",
      "Requirement already satisfied: xxhash<4.0.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from langgraph) (3.5.0)\r\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (0.2.3)\r\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (9.0.0)\r\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (1.33)\r\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (6.0.2)\r\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (24.2)\r\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (4.12.2)\r\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (2.11.0a2)\r\n",
      "Collecting ormsgpack<2.0.0,>=1.8.0 (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph)\r\n",
      "  Downloading ormsgpack-1.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.10/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.28.1)\r\n",
      "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.10/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.12)\r\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.7.1)\r\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (2025.1.31)\r\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.0.7)\r\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10)\r\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.14.0)\r\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.1->langgraph) (3.0.0)\r\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (2.32.3)\r\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (1.0.0)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.1->langgraph) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.1->langgraph) (2.29.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (3.4.1)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (2.3.0)\r\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.3.1)\r\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.2.2)\r\n",
      "Downloading langgraph-0.3.29-py3-none-any.whl (144 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.7/144.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading langgraph_checkpoint-2.0.24-py3-none-any.whl (42 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading langgraph_prebuilt-0.1.8-py3-none-any.whl (25 kB)\r\n",
      "Downloading langgraph_sdk-0.1.61-py3-none-any.whl (47 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading ormsgpack-1.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (223 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.6/223.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: ormsgpack, langgraph-sdk, langgraph-checkpoint, langgraph-prebuilt, langgraph\r\n",
      "Successfully installed langgraph-0.3.29 langgraph-checkpoint-2.0.24 langgraph-prebuilt-0.1.8 langgraph-sdk-0.1.61 ormsgpack-1.9.1\r\n",
      "💬 Final Response:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"customer_analysis\": {\n",
      "    \"language\": \"German\",\n",
      "    \"delivery_within_96h\": null, \n",
      "    \"packaging_condition\": null,\n",
      "    \"correct_shoe_size\": true,\n",
      "    \"material_quality\": \"low\",\n",
      "    \"usage_type\": \"almost daily\",\n",
      "    \"expectations_met\": false\n",
      "  },\n",
      "  \"customer_email\": \"Hallo [Customer Name],\\n\\nvielen Dank für dein Feedback zu deinen KelceTS Schuhen. Es tut uns wirklich leid zu hören, dass du nach nur 2,5 Monaten bereits Probleme mit der Abnutzung hast. Wir legen größten Wert auf Qualität und sind sehr enttäuscht, dass die Schuhe deine Erwartungen nicht erfüllt haben.\\n\\nWir bedauern die entstandenen Unannehmlichkeiten zutiefst.  Um dies zu entschädigen, möchten wir dir einen 25% Rabatt auf deine nächste Bestellung bei KelceTS  gewähren, inklusive kostenlosem Versand. \\n\\nEin Mitarbeiter wird sich innerhalb der nächsten 72 Stunden bei dir melden, um die defekten Schuhe abzuholen. Bitte halte sie dafür in der Originalverpackung oder einer ähnlichen Verpackung bereit.\\n\\nWir bedanken uns für deine Geduld und dein Verständnis.  Wir nehmen dein Feedback sehr ernst und werden unsere Qualitätskontrollen überprüfen, um solche Vorfälle in Zukunft zu vermeiden.\\n\\nMit freundlichen Grüßen,\\nDas KelceTS Team\",\n",
      "  \"internal_email\": \"Betreff: Qualitätsproblem - Kundenservice Rückmeldung\\n\\nTeam,\\n\\nEin Kunde hat sich über die schnelle Abnutzung seiner KelceTS Schuhe beschwert (siehe Kundenrückmeldung unten).\\n\\nDetails:\\n- Schuhe nach 2,5 Monaten (fast täglicher Gebrauch) stark abgenutzt, Löcher in der Sohle, rohes Plastik sichtbar.\\n- Kunde hat die richtige Schuhgröße erhalten.\\n\\nMaßnahmen:\\n- Der Kunde erhält einen 25% Rabatt auf seine nächste Bestellung und kostenlosen Versand.\\n- Ein Mitarbeiter muss die Schuhe innerhalb von 72 Stunden abholen.\\n\\nBitte kontaktiert den Kunden um einen Abholtermin zu vereinbaren.\\n\\n[Kundenrückmeldung einfügen]\\n\\nMit freundlichen Grüßen,\\nDas KelceTS Qualitätsteam\",\n",
      "  \"provider_email\": null\n",
      "}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Install langgraph (only needed once)\n",
    "!pip install langgraph\n",
    "\n",
    "# Import LangGraph tools\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated\n",
    "from langchain_core.runnables import Runnable, RunnableLambda\n",
    "\n",
    "# Define the state structure of the agent\n",
    "class AgentState(TypedDict):\n",
    "    query: str\n",
    "    context: str\n",
    "    response: str\n",
    "\n",
    "# Node 1: retrieve context from vectorstore\n",
    "def retrieve_context(state: AgentState) -> AgentState:\n",
    "    docs = vectorstore.similarity_search(state[\"query\"], k=3)\n",
    "    context = \"\\n\".join([doc.page_content for doc in docs])\n",
    "    return {\"query\": state[\"query\"], \"context\": context}\n",
    "\n",
    "# Node 2: generate response with context + system prompt\n",
    "def generate_response(state: AgentState) -> AgentState:\n",
    "    prompt = f\"\"\"\n",
    "Use the following internal context to answer this customer comment.\n",
    "\n",
    "Context:\n",
    "{state['context']}\n",
    "\n",
    "Customer Comment:\n",
    "{state['query']}\n",
    "\n",
    "Instructions:\n",
    "{system_instruction}\n",
    "\"\"\"\n",
    "    model = genai.GenerativeModel(model_name=\"models/gemini-1.5-flash\")\n",
    "    result = model.generate_content(prompt)\n",
    "    return {\"query\": state[\"query\"], \"context\": state[\"context\"], \"response\": result.text}\n",
    "\n",
    "# Build the LangGraph with two steps\n",
    "graph = StateGraph(AgentState)\n",
    "graph.add_node(\"RetrieveContext\", RunnableLambda(retrieve_context))\n",
    "graph.add_node(\"GenerateResponse\", RunnableLambda(generate_response))\n",
    "\n",
    "graph.set_entry_point(\"RetrieveContext\")\n",
    "graph.add_edge(\"RetrieveContext\", \"GenerateResponse\")\n",
    "graph.set_finish_point(\"GenerateResponse\")\n",
    "\n",
    "# Compile and run the graph\n",
    "agent_executor = graph.compile()\n",
    "\n",
    "# Run with a customer comment\n",
    "result_state = agent_executor.invoke({\"query\": comment_1})\n",
    "\n",
    "# Show final response from LangGraph flow\n",
    "print(\"💬 Final Response:\\n\")\n",
    "print(result_state[\"response\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec32d4f",
   "metadata": {
    "papermill": {
     "duration": 0.011403,
     "end_time": "2025-04-12T13:46:22.937309",
     "exception": false,
     "start_time": "2025-04-12T13:46:22.925906",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 📘 Section 3.3: Visual LangGraph Diagram with Quality Nodes\n",
    "To illustrate the modular architecture of our AI assistant, we visualize an expanded LangGraph that includes quality factor analysis, scoring, and response generation.\n",
    "\n",
    "This graph represents how a production-grade agent could orchestrate its reasoning, structured evaluation, and generation phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb98ff12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T13:46:22.974165Z",
     "iopub.status.busy": "2025-04-12T13:46:22.973780Z",
     "iopub.status.idle": "2025-04-12T13:46:23.450702Z",
     "shell.execute_reply": "2025-04-12T13:46:23.448558Z"
    },
    "papermill": {
     "duration": 0.501563,
     "end_time": "2025-04-12T13:46:23.454332",
     "exception": false,
     "start_time": "2025-04-12T13:46:22.952769",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"250pt\" height=\"332pt\"\n",
       " viewBox=\"0.00 0.00 249.50 332.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 328)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-328 245.5,-328 245.5,4 -4,4\"/>\n",
       "<!-- Input -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>Input</title>\n",
       "<polygon fill=\"lightyellow\" stroke=\"lightyellow\" points=\"168,-324 68,-324 68,-288 168,-288 168,-324\"/>\n",
       "<text text-anchor=\"middle\" x=\"118\" y=\"-302.3\" font-family=\"Times,serif\" font-size=\"14.00\">InputComment</text>\n",
       "</g>\n",
       "<!-- Retrieve -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>Retrieve</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"lightblue\" points=\"170.5,-252 65.5,-252 65.5,-216 170.5,-216 170.5,-252\"/>\n",
       "<text text-anchor=\"middle\" x=\"118\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\">RetrieveContext</text>\n",
       "</g>\n",
       "<!-- Input&#45;&gt;Retrieve -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>Input&#45;&gt;Retrieve</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M118,-287.7C118,-279.98 118,-270.71 118,-262.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"121.5,-262.1 118,-252.1 114.5,-262.1 121.5,-262.1\"/>\n",
       "</g>\n",
       "<!-- Analyze -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>Analyze</title>\n",
       "<polygon fill=\"lightgreen\" stroke=\"lightgreen\" points=\"189,-180 47,-180 47,-144 189,-144 189,-180\"/>\n",
       "<text text-anchor=\"middle\" x=\"118\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\">AnalyzeQualityFactors</text>\n",
       "</g>\n",
       "<!-- Retrieve&#45;&gt;Analyze -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>Retrieve&#45;&gt;Analyze</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M118,-215.7C118,-207.98 118,-198.71 118,-190.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"121.5,-190.1 118,-180.1 114.5,-190.1 121.5,-190.1\"/>\n",
       "</g>\n",
       "<!-- Score -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>Score</title>\n",
       "<polygon fill=\"lightsalmon\" stroke=\"lightsalmon\" points=\"106,-108 0,-108 0,-72 106,-72 106,-108\"/>\n",
       "<text text-anchor=\"middle\" x=\"53\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">ScoreEvaluation</text>\n",
       "</g>\n",
       "<!-- Analyze&#45;&gt;Score -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>Analyze&#45;&gt;Score</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M101.93,-143.7C94.06,-135.22 84.44,-124.86 75.83,-115.58\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"78.25,-113.05 68.88,-108.1 73.12,-117.81 78.25,-113.05\"/>\n",
       "</g>\n",
       "<!-- Generate -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>Generate</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"lightgrey\" points=\"241.5,-108 124.5,-108 124.5,-72 241.5,-72 241.5,-108\"/>\n",
       "<text text-anchor=\"middle\" x=\"183\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">GenerateResponse</text>\n",
       "</g>\n",
       "<!-- Analyze&#45;&gt;Generate -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>Analyze&#45;&gt;Generate</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M134.07,-143.7C141.94,-135.22 151.56,-124.86 160.17,-115.58\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"162.88,-117.81 167.12,-108.1 157.75,-113.05 162.88,-117.81\"/>\n",
       "</g>\n",
       "<!-- End -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>End</title>\n",
       "<ellipse fill=\"lightgrey\" stroke=\"lightgrey\" cx=\"118\" cy=\"-18\" rx=\"28.7\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"118\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">END</text>\n",
       "</g>\n",
       "<!-- Score&#45;&gt;End -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>Score&#45;&gt;End</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M69.07,-71.7C77.49,-62.62 87.92,-51.4 96.98,-41.64\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"99.64,-43.91 103.88,-34.2 94.51,-39.15 99.64,-43.91\"/>\n",
       "</g>\n",
       "<!-- Generate&#45;&gt;End -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>Generate&#45;&gt;End</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M166.93,-71.7C158.51,-62.62 148.08,-51.4 139.02,-41.64\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"141.49,-39.15 132.12,-34.2 136.36,-43.91 141.49,-39.15\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7e918e0b3f10>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "# Create the directed graph\n",
    "dot = Digraph(comment=\"KelceTS LangGraph Pipeline\")\n",
    "\n",
    "# Define nodes\n",
    "dot.node(\"Input\", \"InputComment\", shape=\"box\", style=\"filled\", color=\"lightyellow\")\n",
    "dot.node(\"Retrieve\", \"RetrieveContext\", shape=\"box\", style=\"filled\", color=\"lightblue\")\n",
    "dot.node(\"Analyze\", \"AnalyzeQualityFactors\", shape=\"box\", style=\"filled\", color=\"lightgreen\")\n",
    "dot.node(\"Score\", \"ScoreEvaluation\", shape=\"box\", style=\"filled\", color=\"lightsalmon\")\n",
    "dot.node(\"Generate\", \"GenerateResponse\", shape=\"box\", style=\"filled\", color=\"lightgrey\")\n",
    "dot.node(\"End\", \"END\", shape=\"oval\", style=\"filled\", color=\"lightgrey\")\n",
    "\n",
    "# Define edges\n",
    "dot.edge(\"Input\", \"Retrieve\")\n",
    "dot.edge(\"Retrieve\", \"Analyze\")\n",
    "dot.edge(\"Analyze\", \"Score\")\n",
    "dot.edge(\"Analyze\", \"Generate\")\n",
    "dot.edge(\"Score\", \"End\")\n",
    "dot.edge(\"Generate\", \"End\")\n",
    "\n",
    "# Render inside Kaggle output\n",
    "dot.render(\"kelcets_langgraph_flow\", format=\"png\", cleanup=False)\n",
    "dot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0957b282",
   "metadata": {
    "papermill": {
     "duration": 0.023473,
     "end_time": "2025-04-12T13:46:23.497552",
     "exception": false,
     "start_time": "2025-04-12T13:46:23.474079",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### ✅ Mock node: AnalyzeQualityFactors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21f4b9ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T13:46:23.542522Z",
     "iopub.status.busy": "2025-04-12T13:46:23.542111Z",
     "iopub.status.idle": "2025-04-12T13:46:23.553652Z",
     "shell.execute_reply": "2025-04-12T13:46:23.549866Z"
    },
    "papermill": {
     "duration": 0.041044,
     "end_time": "2025-04-12T13:46:23.559944",
     "exception": false,
     "start_time": "2025-04-12T13:46:23.518900",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Quality Analysis Output:\n",
      "\n",
      "Possible material durability concern.\n"
     ]
    }
   ],
   "source": [
    "# Mock function to simulate analysis of quality-related feedback\n",
    "def analyze_quality_factors(state: AgentState) -> AgentState:\n",
    "    comment = state[\"query\"].lower()\n",
    "    issues = []\n",
    "\n",
    "    if \"größe\" in comment or \"size\" in comment:\n",
    "        issues.append(\"Possible size issue detected.\")\n",
    "    if \"material\" in comment or \"sohle\" in comment or \"materialqualität\" in comment:\n",
    "        issues.append(\"Possible material durability concern.\")\n",
    "    if \"erwartung\" in comment or \"not met\" in comment:\n",
    "        issues.append(\"Expectations may not have been met.\")\n",
    "\n",
    "    summary = \"\\n\".join(issues) if issues else \"No quality concerns detected.\"\n",
    "    return {**state, \"quality_analysis\": summary}\n",
    "\n",
    "# Small test to check it works\n",
    "state = {\"query\": comment_1}\n",
    "output = analyze_quality_factors(state)\n",
    "print(\"🧪 Quality Analysis Output:\\n\")\n",
    "print(output[\"quality_analysis\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052498b7",
   "metadata": {
    "papermill": {
     "duration": 0.021117,
     "end_time": "2025-04-12T13:46:23.605524",
     "exception": false,
     "start_time": "2025-04-12T13:46:23.584407",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 📘 Section 4: Create a Test Message and Interpret Output\n",
    "In this section, we take the output generated by Gemini and:\n",
    "\n",
    "\n",
    "* Parse the JSON (if well-formatted)\n",
    "* Extract the key elements of the response\n",
    "* Present a summary table (optional)\n",
    "* Prepare for structured evaluation or export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c5f4e53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T13:46:23.631721Z",
     "iopub.status.busy": "2025-04-12T13:46:23.631206Z",
     "iopub.status.idle": "2025-04-12T13:46:23.643355Z",
     "shell.execute_reply": "2025-04-12T13:46:23.641991Z"
    },
    "papermill": {
     "duration": 0.026931,
     "end_time": "2025-04-12T13:46:23.645324",
     "exception": false,
     "start_time": "2025-04-12T13:46:23.618393",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ JSON parsed successfully!\n",
      "\n",
      "📦 Customer Analysis:\n",
      " {'language': 'German', 'delivery_within_96h': 'Not specified', 'packaging_condition': 'Not specified', 'correct_shoe_size': 'Yes', 'material_quality': 'Poor', 'usage_type': 'Daily', 'expectations_met': 'No'}\n",
      "\n",
      "📧 Customer Email:\n",
      " Hallo! Vielen Dank für dein Feedback. Es tut uns sehr leid zu hören, dass deine Schuhe sich so schnell abgenutzt haben. Wir legen großen Wert auf die Qualität unserer Produkte und bedauern, dass deine Erwartungen nicht erfüllt wurden.  Wir möchten dies gerne wieder gutmachen. Bitte sende uns Fotos der beschädigten Stellen an info@kelcets.com. Wir werden dir dann umgehend ein neues Paar Schuhe zusenden oder dir den Kaufpreis erstatten.  Vielen Dank für dein Verständnis.\n",
      "\n",
      "📨 Internal Email:\n",
      " Betreff: Kundenbeschwerde - Schnelle Abnutzung der Schuhe\n",
      "\n",
      "Kundennummer: [Kundennummer einfügen]\n",
      "\n",
      "Ein Kunde hat sich über die schnelle Abnutzung seiner Schuhe beschwert. Er gab an, die Schuhe täglich getragen zu haben, und nach nur 2,5 Monaten seien Löcher in den Sohlen und abgenutzte Stellen im Inneren sichtbar.  Der Kunde erwartet eine Ersatzlieferung oder eine Rückerstattung. Bitte prüfen Sie den Fall und veranlassen Sie die entsprechende Aktion. Fotos der Schuhe wurden vom Kunden angefordert und werden nach Erhalt an Sie weitergeleitet.\n",
      "\n",
      "📤 Provider Email:\n",
      " Sehr geehrte Damen und Herren,\n",
      "\n",
      "wir haben eine Kundenbeschwerde bezüglich der Haltbarkeit eines Ihrer Produkte erhalten. Der Kunde berichtet von übermäßigem Verschleiß und Materialermüdung nach nur 2,5 Monaten täglicher Nutzung.  Anbei finden Sie Fotos der beschädigten Schuhe [Fotos anhängen].  Wir bitten Sie, diesen Fall zu untersuchen und uns Ihre Einschätzung zur Ursache des Problems mitzuteilen.  Wir erwarten eine Stellungnahme Ihrerseits, wie wir in Zukunft ähnliche Qualitätsprobleme vermeiden können.  Mit freundlichen Grüßen,\n",
      "\n",
      "KelceTS S.L. Qualitätsmanagement\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Clean the response: remove Markdown-style ```json wrapping if it exists\n",
    "cleaned_text = response.text.strip()\n",
    "if cleaned_text.startswith(\"```json\"):\n",
    "    cleaned_text = cleaned_text.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "\n",
    "# Try parsing the cleaned text\n",
    "try:\n",
    "    result = json.loads(cleaned_text)\n",
    "    print(\"✅ JSON parsed successfully!\\n\")\n",
    "\n",
    "    # Display sections\n",
    "    print(\"📦 Customer Analysis:\\n\", result.get(\"customer_analysis\", \"Not found\"))\n",
    "    print(\"\\n📧 Customer Email:\\n\", result.get(\"customer_email\", \"Not found\"))\n",
    "    print(\"\\n📨 Internal Email:\\n\", result.get(\"internal_email\", \"Not found\"))\n",
    "    print(\"\\n📤 Provider Email:\\n\", result.get(\"provider_email\", \"Not found\"))\n",
    "\n",
    "except json.JSONDecodeError:\n",
    "    print(\"⚠️ The cleaned response is still not valid JSON. Showing raw output below:\\n\")\n",
    "    print(cleaned_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b9a0b6",
   "metadata": {
    "papermill": {
     "duration": 0.013075,
     "end_time": "2025-04-12T13:46:23.671522",
     "exception": false,
     "start_time": "2025-04-12T13:46:23.658447",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 📘 Section 5: Export Output to Table or Excel\n",
    "In this section, we convert the structured JSON output into a clean tabular format. This allows:\n",
    "\n",
    "* Easy export to Excel\n",
    "* Further statistical analysis\n",
    "* Internal quality reporting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60c65099",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T13:46:23.698974Z",
     "iopub.status.busy": "2025-04-12T13:46:23.698420Z",
     "iopub.status.idle": "2025-04-12T13:46:23.759545Z",
     "shell.execute_reply": "2025-04-12T13:46:23.758171Z"
    },
    "papermill": {
     "duration": 0.077274,
     "end_time": "2025-04-12T13:46:23.761794",
     "exception": false,
     "start_time": "2025-04-12T13:46:23.684520",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Customer analysis table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>delivery_within_96h</th>\n",
       "      <th>packaging_condition</th>\n",
       "      <th>correct_shoe_size</th>\n",
       "      <th>material_quality</th>\n",
       "      <th>usage_type</th>\n",
       "      <th>expectations_met</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>German</td>\n",
       "      <td>Not specified</td>\n",
       "      <td>Not specified</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Poor</td>\n",
       "      <td>Daily</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  language delivery_within_96h packaging_condition correct_shoe_size  \\\n",
       "0   German       Not specified       Not specified               Yes   \n",
       "\n",
       "  material_quality usage_type expectations_met  \n",
       "0             Poor      Daily               No  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Full response table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>Delivery within 96h?</th>\n",
       "      <th>Shoe size correct?</th>\n",
       "      <th>Material quality</th>\n",
       "      <th>Usage type</th>\n",
       "      <th>Expectations met</th>\n",
       "      <th>Customer email</th>\n",
       "      <th>Internal email</th>\n",
       "      <th>Provider email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>German</td>\n",
       "      <td>Not specified</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Poor</td>\n",
       "      <td>Daily</td>\n",
       "      <td>No</td>\n",
       "      <td>Hallo! Vielen Dank für dein Feedback. Es tut u...</td>\n",
       "      <td>Betreff: Kundenbeschwerde - Schnelle Abnutzung...</td>\n",
       "      <td>Sehr geehrte Damen und Herren,\\n\\nwir haben ei...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Language Delivery within 96h? Shoe size correct? Material quality  \\\n",
       "0   German        Not specified                Yes             Poor   \n",
       "\n",
       "  Usage type Expectations met  \\\n",
       "0      Daily               No   \n",
       "\n",
       "                                      Customer email  \\\n",
       "0  Hallo! Vielen Dank für dein Feedback. Es tut u...   \n",
       "\n",
       "                                      Internal email  \\\n",
       "0  Betreff: Kundenbeschwerde - Schnelle Abnutzung...   \n",
       "\n",
       "                                      Provider email  \n",
       "0  Sehr geehrte Damen und Herren,\\n\\nwir haben ei...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Exported as kelcets_feedback_output.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Convert the extracted \"customer_analysis\" part into a DataFrame\n",
    "if \"customer_analysis\" in result:\n",
    "    analysis_df = pd.DataFrame([result[\"customer_analysis\"]])\n",
    "    print(\"✅ Customer analysis table:\")\n",
    "    display(analysis_df)\n",
    "else:\n",
    "    print(\"⚠️ No 'customer_analysis' section found in the result.\")\n",
    "\n",
    "# Step 2: Combine the full JSON into one flat table (including emails)\n",
    "flat_data = {\n",
    "    \"Language\": result.get(\"customer_analysis\", {}).get(\"language\", \"\"),\n",
    "    \"Delivery within 96h?\": result.get(\"customer_analysis\", {}).get(\"delivery_within_96h\", \"\"),\n",
    "    \"Shoe size correct?\": result.get(\"customer_analysis\", {}).get(\"correct_shoe_size\", \"\"),\n",
    "    \"Material quality\": result.get(\"customer_analysis\", {}).get(\"material_quality\", \"\"),\n",
    "    \"Usage type\": result.get(\"customer_analysis\", {}).get(\"usage_type\", \"\"),\n",
    "    \"Expectations met\": result.get(\"customer_analysis\", {}).get(\"expectations_met\", \"\"),\n",
    "    \"Customer email\": result.get(\"customer_email\", \"\"),\n",
    "    \"Internal email\": result.get(\"internal_email\", \"\"),\n",
    "    \"Provider email\": result.get(\"provider_email\", \"\")\n",
    "}\n",
    "\n",
    "df_export = pd.DataFrame([flat_data])\n",
    "\n",
    "# Step 3: Display the full table\n",
    "print(\"📊 Full response table:\")\n",
    "display(df_export)\n",
    "\n",
    "# Step 4: Optional – Export to Excel\n",
    "df_export.to_csv(\"kelcets_feedback_output.csv\", index=False)\n",
    "print(\"📁 Exported as kelcets_feedback_output.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e9fc73",
   "metadata": {
    "papermill": {
     "duration": 0.013055,
     "end_time": "2025-04-12T13:46:23.787643",
     "exception": false,
     "start_time": "2025-04-12T13:46:23.774588",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5.1: Enable File Download\n",
    "This cell allows you to download the generated CSV file with the processed feedback and AI-generated responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb947761",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T13:46:23.815466Z",
     "iopub.status.busy": "2025-04-12T13:46:23.814960Z",
     "iopub.status.idle": "2025-04-12T13:46:23.823668Z",
     "shell.execute_reply": "2025-04-12T13:46:23.822458Z"
    },
    "papermill": {
     "duration": 0.024985,
     "end_time": "2025-04-12T13:46:23.825680",
     "exception": false,
     "start_time": "2025-04-12T13:46:23.800695",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⬇️ Click below to download the result:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='kelcets_feedback_output.csv' target='_blank'>kelcets_feedback_output.csv</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/kelcets_feedback_output.csv"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "\n",
    "# Display download link for the exported CSV\n",
    "print(\"⬇️ Click below to download the result:\")\n",
    "FileLink(\"kelcets_feedback_output.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5d50e8",
   "metadata": {
    "papermill": {
     "duration": 0.013721,
     "end_time": "2025-04-12T13:46:23.852467",
     "exception": false,
     "start_time": "2025-04-12T13:46:23.838746",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 📘 Section 6: Automatic Evaluation of Agent Responses\n",
    "This section implements an evaluation framework for assessing the quality of the AI assistant’s output based on three key dimensions:\n",
    "\n",
    "\n",
    "* Groundedness (based on original comment)\n",
    "* Relevance (to the complaint or praise)\n",
    "* Empathy & Tone (brand-aligned and appropriate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08ed54b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T13:46:23.880786Z",
     "iopub.status.busy": "2025-04-12T13:46:23.880265Z",
     "iopub.status.idle": "2025-04-12T13:46:23.889449Z",
     "shell.execute_reply": "2025-04-12T13:46:23.888020Z"
    },
    "papermill": {
     "duration": 0.026389,
     "end_time": "2025-04-12T13:46:23.891629",
     "exception": false,
     "start_time": "2025-04-12T13:46:23.865240",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Simulated evaluation rubric\n",
    "def evaluate_agent_response(comment, response_json):\n",
    "    feedback = {}\n",
    "\n",
    "    # Check 1: Groundedness (does it address what the user actually said?)\n",
    "    feedback[\"groundedness\"] = \"yes\" if \"abnutzen\" in comment and \"abgenutzt\" in response_json.get(\"customer_email\", \"\") else \"no\"\n",
    "\n",
    "    # Check 2: Relevance (does it respond to key issues like defects, delay, size, etc.)\n",
    "    keywords = [\"Größe\", \"Qualität\", \"Material\", \"Verzögerung\", \"Erwartungen\"]\n",
    "    hits = sum([1 for k in keywords if k.lower() in response_json.get(\"customer_email\", \"\").lower()])\n",
    "    feedback[\"relevance\"] = \"high\" if hits >= 2 else \"low\"\n",
    "\n",
    "    # Check 3: Tone (is the tone empathetic, brand-aligned and uses friendly form?)\n",
    "    if \"tut uns sehr leid\" in response_json.get(\"customer_email\", \"\") and \"danke\" in response_json.get(\"customer_email\", \"\").lower():\n",
    "        feedback[\"empathy_tone\"] = \"strong\"\n",
    "    else:\n",
    "        feedback[\"empathy_tone\"] = \"weak\"\n",
    "\n",
    "    return feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12085831",
   "metadata": {
    "papermill": {
     "duration": 0.013973,
     "end_time": "2025-04-12T13:46:23.918368",
     "exception": false,
     "start_time": "2025-04-12T13:46:23.904395",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 🔍 6.1 Apply the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20540096",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T13:46:23.946859Z",
     "iopub.status.busy": "2025-04-12T13:46:23.946359Z",
     "iopub.status.idle": "2025-04-12T13:46:23.954378Z",
     "shell.execute_reply": "2025-04-12T13:46:23.952922Z"
    },
    "papermill": {
     "duration": 0.023862,
     "end_time": "2025-04-12T13:46:23.956788",
     "exception": false,
     "start_time": "2025-04-12T13:46:23.932926",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Evaluation of the AI agent's output:\n",
      "\n",
      "Groundedness: yes\n",
      "Relevance: high\n",
      "Empathy_Tone: weak\n"
     ]
    }
   ],
   "source": [
    "# Apply evaluation to the previously loaded response\n",
    "evaluation = evaluate_agent_response(comment_1, result)\n",
    "\n",
    "# Show results\n",
    "print(\"📈 Evaluation of the AI agent's output:\\n\")\n",
    "for key, value in evaluation.items():\n",
    "    print(f\"{key.title()}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c41ad4",
   "metadata": {
    "papermill": {
     "duration": 0.012226,
     "end_time": "2025-04-12T13:46:23.981996",
     "exception": false,
     "start_time": "2025-04-12T13:46:23.969770",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 📝 6.2 Optional: Display evaluation as table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9291cab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T13:46:24.009661Z",
     "iopub.status.busy": "2025-04-12T13:46:24.009295Z",
     "iopub.status.idle": "2025-04-12T13:46:24.021416Z",
     "shell.execute_reply": "2025-04-12T13:46:24.020189Z"
    },
    "papermill": {
     "duration": 0.02785,
     "end_time": "2025-04-12T13:46:24.023488",
     "exception": false,
     "start_time": "2025-04-12T13:46:23.995638",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Evaluation Summary Table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>groundedness</th>\n",
       "      <th>relevance</th>\n",
       "      <th>empathy_tone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yes</td>\n",
       "      <td>high</td>\n",
       "      <td>weak</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  groundedness relevance empathy_tone\n",
       "0          yes      high         weak"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_df = pd.DataFrame([evaluation])\n",
    "print(\"📊 Evaluation Summary Table:\")\n",
    "display(eval_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b388991d",
   "metadata": {
    "papermill": {
     "duration": 0.012493,
     "end_time": "2025-04-12T13:46:24.048871",
     "exception": false,
     "start_time": "2025-04-12T13:46:24.036378",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ✅Section 6.2: Compare Standard vs Retrieval Augmented Response\n",
    "This section compares the quality of the assistant’s original response with the response generated using policy-grounded context (RAG).\n",
    "\n",
    "We apply the same evaluation function to both versions and display the scores side by side for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c13ad672",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T13:46:24.076061Z",
     "iopub.status.busy": "2025-04-12T13:46:24.075546Z",
     "iopub.status.idle": "2025-04-12T13:46:24.094105Z",
     "shell.execute_reply": "2025-04-12T13:46:24.092919Z"
    },
    "papermill": {
     "duration": 0.034171,
     "end_time": "2025-04-12T13:46:24.095841",
     "exception": false,
     "start_time": "2025-04-12T13:46:24.061670",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Evaluation Comparison Table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>version</th>\n",
       "      <th>groundedness</th>\n",
       "      <th>relevance</th>\n",
       "      <th>empathy_tone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Standard</td>\n",
       "      <td>yes</td>\n",
       "      <td>high</td>\n",
       "      <td>weak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RAG-enhanced</td>\n",
       "      <td>yes</td>\n",
       "      <td>low</td>\n",
       "      <td>strong</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        version groundedness relevance empathy_tone\n",
       "0      Standard          yes      high         weak\n",
       "1  RAG-enhanced          yes       low       strong"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate both responses: standard (result) and RAG-based (response_rag)\n",
    "# First, extract the JSON from the RAG version (clean markdown formatting if needed)\n",
    "import json\n",
    "\n",
    "# Clean RAG response and parse\n",
    "cleaned_rag_text = response_rag.text.strip()\n",
    "if cleaned_rag_text.startswith(\"```json\"):\n",
    "    cleaned_rag_text = cleaned_rag_text.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "\n",
    "# Try parsing RAG response as JSON\n",
    "try:\n",
    "    result_rag = json.loads(cleaned_rag_text)\n",
    "except json.JSONDecodeError:\n",
    "    result_rag = {\"customer_email\": \"\", \"internal_email\": \"\", \"provider_email\": \"\"}\n",
    "\n",
    "# Evaluate both versions\n",
    "evaluation_standard = evaluate_agent_response(comment_1, result)\n",
    "evaluation_rag = evaluate_agent_response(comment_1, result_rag)\n",
    "\n",
    "# Combine into comparison table\n",
    "comparison_df = pd.DataFrame([\n",
    "    {\"version\": \"Standard\", **evaluation_standard},\n",
    "    {\"version\": \"RAG-enhanced\", **evaluation_rag}\n",
    "])\n",
    "\n",
    "# Show the comparison\n",
    "print(\"📊 Evaluation Comparison Table:\")\n",
    "display(comparison_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76484fba",
   "metadata": {
    "papermill": {
     "duration": 0.012721,
     "end_time": "2025-04-12T13:46:24.121551",
     "exception": false,
     "start_time": "2025-04-12T13:46:24.108830",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 📘 Section 7: Conclusions and Future Work\n",
    "This project demonstrates a practical application of Generative AI for improving customer service operations in a fictional e-commerce startup, **KelceTS S.L.**. Using a real-world use case—automated processing and response to customer feedback—we built an AI agent that:\n",
    "\n",
    "- Analyzes multilingual comments using structured output\n",
    "- Responds to the customer, internal teams, and suppliers\n",
    "- Aligns with internal rules through retrieval-augmented generation (RAG)\n",
    "- Evaluates its own output across key quality dimensions\n",
    "\n",
    "By combining core GenAI capabilities—prompt engineering, structured output, embeddings, and RAG—the solution delivers scalable, policy-compliant responses. It’s suitable for integration into back-office tools or CRM workflows.\n",
    "\n",
    "\n",
    "## 🚀 Future Improvements\n",
    "\n",
    "- Use larger customer datasets to fine-tune responses at scale\n",
    "- Integrate multimodal support (images of damaged items, voice complaints)\n",
    "- Add a feedback loop using human rating to refine evaluation criteria\n",
    "- Deploy via API for real-time processing in CRM systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87febfcc",
   "metadata": {
    "papermill": {
     "duration": 0.012918,
     "end_time": "2025-04-12T13:46:24.147645",
     "exception": false,
     "start_time": "2025-04-12T13:46:24.134727",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "# 📘 Section 8: Submission Summary & Acknowledgements\n",
    "\n",
    "Thanks to Kaggle and Google for this challenge. This project shows how GenAI can analyze multilingual customer feedback, align with internal policies using RAG, and structure workflows with LangGraph.\n",
    "\n",
    "Made from Móstoles, Madrid ,Spain with ❤️ by Araceli Fradejas Muñoz."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7118793,
     "sourceId": 11371481,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 87.324513,
   "end_time": "2025-04-12T13:46:26.998245",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-12T13:44:59.673732",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
